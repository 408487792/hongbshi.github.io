<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[水平触发与边沿触发]]></title>
    <url>%2F2018%2F11%2F05%2FLT-ET%2F</url>
    <content type="text"><![CDATA[一. 基础1.1 水平触发 基本概念 读缓冲区不为空时, 读事件触发。 写缓冲区不为满时, 写事件触发。 处理流程 accept新的连接, 监听读事件。 读事件到达, 处理读事件。 需要写入数据, 向fd中写数据, 一次无法写完, 开启写事件监听。 写事件到达, 继续写入数据, 写完后关闭写事件。 优缺点 不会遗漏事件, 易编程。 长连接需要写入的数据量大时, 会频繁开启关闭写事件。 1.2 边沿触发 基本概念 读缓冲区状态变化时, 读事件触发, 网卡接受到新数据。 写缓冲区状态变化时, 写事件触发, 网卡发出了新数据。 处理流程 accept新的连接, 同时监听读写事件。 读事件到达, 需要一直读取数据, 直到返回EAGAIN。 写事件到达, 无数据处理则不处理, 有数据待写入则一直写入，直到写完或者返回EAGAIN。 优缺点 不需要频繁开启关闭事件, 效率较高。 读写事件处理不当, 可能导致事件丢失, 编程教复杂。 1.3 选择 概述 对于读事件而言，总体而言, 采用水平触发方式较好。应用程序在读取数据时，可能会一次无法读取全部数据，边沿触发在下一次可能不会触发。如果能够保证一次读取缓存的全部数据，可以采用边沿触发，效率更高, 但同时编程复杂度也高。 对于写事件，当客户端服务端采用短连接或者采用长连接但发送的数据量比较少时(例如: Redis), 采用水平触发即可。当客户端与服务端是长连接并且数据写入的量比较大时(例如: nginx), 采用边沿触发, 因为边沿触发效率更高。 目前，linux不支持读写事件分别设置不同的触发方式，具体采用哪种方式触发，需要根据具体需求。 监听套接字事件设置 监听套接字不需要监听写事件，只需要监听读事件。 监听套接字一般采用水平触发方式。(nginx开启multi_accept时，会把监听套接字所有可读的事件全部读取，此时可以使用边沿触发。但为了保证连接不丢失，nginx仍然采用水平触发) 通信套接字设置 redis对于与客户端通信使用的套接字默认使用水平触发。 nginx对于与客户端通信使用的套接字默认采用边沿触发。 二. 参考 https://blog.csdn.net/dongfuye/article/details/50880251]]></content>
      <categories>
        <category>网络编程</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F11%2F01%2Fload_balance%2F</url>
    <content type="text"><![CDATA[一. 基础知识1.1 基础 什么是负载均衡? 当单机提供的并发量不能满足需求时，我们需要多台服务器同时服务。当客户请求到达时，如何为客户选择最合适的服务器?这个问题就是负载均衡问题。 负载均衡主要需要解决的问题是哪些? 从客户端的角度上看，客户需要最快速的得到服务器的相应，负载均衡时需要找出能最快相应客户需求的服务器进行服务。 从服务端来看如何使得每台服务器都能达到较高的利用率，最大限制的为用户提供快速、可靠的服务是服务端需要考虑的主要问题。 1.2 负载均衡分类 硬件 F5 软件 dns负载均衡 LVS负载均衡(4层) nginx, haproxy(7层) 二. F5负载均衡 F5是一家美国的公司，该公司生产一些硬件设备可以作为负载均衡器使用(例如:big-ip), 本文后续部分所说的F5是指其负载均衡器产品。 不同的产品实现的功能不一致，具体情况需要根据产品说明书。 F5可以在4-7层内做负载均衡，用户可以根据需求进行配置。 由于F5可以做7层负载均衡，故而可以实现会话管理，http处理等。 2.1 数据转发模式 standard类型, 这种模式下，客户端与F5服务器建立连接，F5服务器与真实服务器建立连接，F5服务器将客户需求转发给真实服务器，并将真实服务器的相应转发给客户端，此时F5可以查看请求和相应的所有信息。 四层转发模式(performance L4), 这种模式下，F5只处理4层以下的数据。客户端将数据发送给F5, F5仅将数据转发给真实服务器，包括TCP的握手数据包以及挥手数据包，真实服务器需要先将数据发送给F5服务器，F5将其转发给客户端。 路由模式, 这种模式与LVS的DR模式类似。 … 2.2 负载均衡算法 轮询，加权轮询。 源地址哈希 … 2.3 小结F5的优势在于功能强大，并发量高，能满足客户的大多数需求，但其成本较高，一般大型国企可能会使用。 2.4 参考 https://f5.com/zh https://www.jianshu.com/p/2b55aa4c21e2 https://wenku.baidu.com/view/450b8643cc7931b765ce15c1.html 三. dns负载均衡 dns负载均衡由dns服务提供厂商提供。 最初的dns负载均衡提供简单轮询，不能根据客户端或者服务端状态进行选择。 目前，有些dns服务厂商可以提供智能dns服务，用户可以设置负载均衡方案，例如：根据客户端ip地址，选择就近的服务器。 对于目前大多数的公司而言，为了更好的服务用户，通常会使用dns负载均衡，将用户按照就近原则，分配到某个集群服务器上。之后，集群内再采用其他的负载均衡方案。 四. Linux Virtual Server(LVS) LVS通过修改数据包Ip地址，Mac地址实现负载均衡。 LVS由ipvs(内核中), ipvsadm(用户态)组成。LVS需要理解tcp，ip头部。 当tcp握手信号，SYN数据包达到时，ipvs选择一个后端服务器，将数据包进行转发。在此之后，所有包含相同的ip，tcp头部的数据包都会被转发到之前选择的服务器上。很明显，ipvs无法感知数据包内容。 4.1 分类 LVS-NAT LVS-DR LVS-TUN 4.2 基本原理4.2.1 LVS-DRLVS-DR模式的基本原理如下图所示: 4.2.2 LVS-NATLVS-NAT模式的基本原理如下图所示: 4.3 负载均衡算法4.3.1 静态算法 轮询(Round Robin, RR) 加权轮询(Weight Round Robin, WRR) 源地址Hash(Source Hash, SH) 目的地址Hash(Destination Hash, DH), 可以设置多个VIP 4.3.2 动态算法 最少连接(Least Connections, LC)，找出当前连接数最小的服务器 加权最少连接(Weighted Least Connections, WLC) 最短期望延迟(Shortest Expected Delay Scheduling, SED) 基于WLC。例如: 现有A, B, C三台服务器，权重分别为100,200,300，当前的连接数分别为1,2,3,下一个连接到达时，通过计算期望时延选择服务器(1+1)/100, (2+1)/200, (3+1)/300, 故而选择C服务器。 永不排队(Never Queue Scheduling, NQ)， 改进的sed, 如果某台服务器连接数为0，直接连接过去，不在进行sed计算。 基于局部性的最少连接(locality-Based Least Connections, LBLC)，根据目标ip, 找出目标ip最近使用的服务器，如果服务器存在并且负载没有大于一个阈值，则将新的连接分配到这个服务器上，否则按照最少连接找出一个服务器处理该请求。 带复制的基于局部性最少连接(Locality-Based Least Connections with Replication, LBLCR)，根据目标ip，维护一个服务器组，每次从组中挑选服务器，如果服务器不可以处理，则从所有服务器中按照最少连接挑选出一台服务器，并将其加入到目标ip的处理组服务器中。 4.3 参考 https://liangshuang.name/2017/11/19/lvs/ 五. Nginx Load Balance nginx负载均衡工作在7层，它会与client、upstream分别建立tcp连接，nginx需要维护这两个连接的状态。 nginx的stream模块可以用于4层负载均衡，但一般很少使用。 5.1 基本原理nginx做7层负载均衡的基本原理如下图所示: 5.2 负载均衡算法 轮询(默认) 加权轮询 源ip哈希 响应时间 url 哈希]]></content>
      <categories>
        <category>架构</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>分布式技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx内存管理]]></title>
    <url>%2F2018%2F10%2F31%2Fnginx_memory_manage%2F</url>
    <content type="text"><![CDATA[一. 概述 应用程序的内存可以简单分为堆内存，栈内存。对于栈内存而言，在函数编译时，编译器会插入移动栈当前指针位置的代码，实现栈空间的自管理。而对于堆内存，通常需要程序员进行管理。我们通常说的内存管理亦是只堆空间内存管理。 对于内存，我们的使用可以简化为3步，申请内存、使用内存、释放内存。申请内存，使用内存通常需要程序员显示操作，释放内存却并不一定需要程序员显示操作，目前很多的高级语言提供了垃圾回收机制，可以自行选择时机释放内存，例如: Go、Java已经实现垃圾回收, C语言目前尚未实现垃圾回收，C++中可以通过智能指针达到垃圾回收的目的。 除了语言层面的内存管理外，有时我们需要在程序中自行管理内存，总体而言，对于内存管理，我认为主要是解决以下问题: 用户申请内存时，如何快速查找到满足用户需求的内存块？ 用户释放内存时，如何避免内存碎片化？ 无论是语言层面实现的内存管理还是应用程序自行实现的内存管理，大都将内存按照大小分为几种，每种采用不同的管理模式。常见的分类是按照2的整数次幂分，将不同种类的内存通过链表链接，查询时，从相应大小的链表中寻找，如果找不到，则可以考虑从更大块内存中，拿取一块，将其分为多个小点的内存。当然，对于特别大的内存，语言层面的内存管理可以直接调用内存管理相关的系统调用，应用层面的内存管理则可以直接使用语言层面的内存管理。 nginx内存管理整体可以分为2个部分， 第一部分是常规的内存池，用于进程平时所需的内存管理； 第二部分是共享内存的管理。总体而言，共享内存教内存池要复杂的多。 二. nginx内存池管理2.1 说明 本部分使用的nginx版本为1.15.3 具体源码参见src/core/ngx_palloc.c文件 2.2 nginx实现2.2.1 使用流程nginx内存池的使用较为简单,可以分为3步， 调用ngx_create_pool函数获取ngx_pool_t指针。 12//size代表ngx_pool_t一块的大小ngx_pool_t* ngx_create_pool(size_t size, ngx_log_t *log) 调用ngx_palloc申请内存使用 12//从pool中申请size大小的内存void* ngx_palloc(ngx_pool_t *pool, size_t size) 释放内存(可以释放大块内存或者释放整个内存池) 1234//释放从pool中申请的大块内存ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p)//释放整个内存池void ngx_destroy_pool(ngx_pool_t *pool) 2.2.2 具体实现 如下图所示，nginx将内存分为2种，一种是小内存，一种是大内存，当申请的空间大于pool-&gt;max时，我们认为是大内存空间，否则是小内存空间。12//创建内存池的参数size减去头部管理结构ngx_pool_t的大小pool-&gt;max = size - sizeof(ngx_pool_t); 对于小块内存空间, nginx首先查看当前内存块待分配的空间中，是否能够满足用户需求，如果可以，则直接将这部分内存返回。如果不能满足用户需求，则需要重新申请一个内存块，申请的内存块与当前块空间大小相同，将新申请的内存块通过链表链接到上一个内存块，从新的内存块中分配用户所需的内存。 小块内存并不释放，用户申请后直接使用，即使后期不再使用也不需要释放该内存。由于用户有时并不知道自己使用的内存块是大是小，此时也可以调用ngx_pfree函数释放该空间，该函数会从大空间链表中查找内存，找到则释放内存。对于小内存而言，并未做任何处理。 对于大块内存, nginx会将这些内存放到链表中存储，通过pool-&gt;large进行管理。值得注意的是，用户管理大内存的ngx_pool_large_t结构是从本内存池的小块内存中申请而来，也就意味着无法释放这些内存，nginx则是直接复用ngx_pool_large_t结构体。当用户需要申请大内存空间时，利用c函数库malloc申请空间，然后将其挂载某个ngx_pool_large_t结构体上。nginx在需要一个新的ngx_pool_large_t结构时，会首先pool-&gt;large链表的前3个元素中，查看是否有可用的,如果有则直接使用，否则新建ngx_pool_large_t结构。 三. nginx共享内存管理3.1 说明 本部分使用的nginx版本是1.15.3 本部分源码详见src/core/ngx_slab.c, src/core/ngx_shmtx.c nginx共享内存内容相对较多，本文仅做简单概述。 3.2 直接使用共享内存3.2.1 基础 nginx中需要创建互斥锁，用于后面多进程同步使用。除此之外，nginx可能需要一些统计信息，例如设置(stat_stub),对于这些变量，我们并不需要特意管理，只需要开辟共享空间后，直接使用即可。 设置stat_stub后所需的统计信息，亦是放到共享内存中，我们此处仅以nginx中的互斥锁进行说明。 3.2.2 nginx互斥锁的实现 nginx互斥锁，有两种方案，当系统支持原子操作时，采用原子操作，不支持时采用文件锁。本节源码见ngx_event_module_init函数。 下图为文件锁实现互斥锁的示意图。 下图为原子操作实现互斥锁的示意图。 问题 reload时，新启动的master向老的master发送信号后直接退出，旧的master,重新加载配置(ngx_init_cycle函数), 新创建工作进程, 新的工作进程与旧的工作进程使用的锁是相同的。 平滑升级时, 旧的master会创建新的master, 新的master会继承旧的master监听的端口(通过环境变量传递监听套接字对应的fd)，新的进程并没有重新绑定监听端口。可能存在新老worker同时监听某个端口的情况，此时操作系统会保证只会有一个进程处理该事件(虽然epoll_wait都会被唤醒)。 3.3 通过slab管理共享内存 nginx允许各个模块开辟共享空间以供使用,例如ngx_http_limit_conn_module模块。 nginx共享内存管理的基本思想有: 将内存按照页进行分配，每页的大小相同, 此处设为page_size。 将内存块按照2的整数次幂进行划分, 最小为8bit, 最大为page_size/2。例如，假设每页大小为4Kb, 则将内存分为8, 16, 32, 64, 128, 256, 512, 1024, 2048共9种，每种对应一个slot, 此时slots数组的大小n即为9。申请小块内存(申请内存大小size &lt;= page_size/2)时，直接给用户这9种中的一种，例如，需要30bit时，找大小为32的内存块提供给用户。 每个页只会划分一种类型的内存块。例如，某次申请内存时，现有内存无法满足要求，此时会使用一个新的页，则这个新页此后只会分配这种大小的内存。 通过双向链表将所有空闲的页连接。图中ngx_slab_pool_t中的free变量即使用来链接空闲页的。 通过slots数组将所有小块内存所使用的页链接起来。 对于大于等于页面大小的空间请求，计算所需页数，找到连续的空闲页，将空闲页的首页地址返回给客户使用，通过每页的管理结构ngx_slab_page_t进行标识。 所有页面只会有3中状态，空闲、未满、已满。空闲，未满都是通过双向链表进行整合，已满页面则不存在与任何页面，当空间被释放时，会将其加入到某个链表。 nginx共享内存的基本结构图如下: 在上图中，除了最右侧的ngx_slab_pool_t接口开始的一段内存位于共享内存区外，其他内存都不是共享内存。 共享内存最终是从page中分配而来。]]></content>
      <categories>
        <category>nginx</category>
        <category>内存管理</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
